# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c688KgZ16QdpL5iG915WchqnJ3zj8AVb

# Anime Recommendation System

## Import Library

Pertama dilakukan beberapa library yang digunakan seperti Numpy, Matplotlib, Pandas, Seaborn, Scikit-learn, dan Tensorflow.
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity

"""## Data Loading

Langkah ini digunakan untuk memuat dataset dari situs Kaggle
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("CooperUnion/anime-recommendations-database")

print("Path to dataset files:", path)

anime = pd.read_csv('/kaggle/input/anime-recommendations-database/anime.csv')
rating = pd.read_csv('/kaggle/input/anime-recommendations-database/rating.csv')

print("total anime:", len(anime.anime_id.unique()))
print("total user:", len(rating.user_id.unique()))
print("total anime yang diberi rating:", len(rating.anime_id.unique()))

"""## Univariate Exploratory Data Analysis

Bagian ini adalah untuk menganalisis struktur data misalnya jumlah baris, kolom, baris duplikat, missing value, dan melihat data melalui statistik deskriptif

### Anime
"""

anime.info()

anime.describe()

anime

# cek duplikat
anime.duplicated().sum()

# cek missing value
anime.isnull().sum()

"""Data anime memiliki beberapa missing value"""

# Atur ukuran plot
plt.figure(figsize=(12, 6))

# Buat heatmap untuk menunjukkan nilai null
sns.heatmap(anime.isnull(),
            cmap='viridis',
            cbar=False,
            yticklabels=False
           )

plt.title("Heatmap Nilai Null dalam DataFrame Anime")
plt.show()

anime.hist(bins=50, figsize=(20,15))
plt.show()

"""dari grafik tersebut diketahui bahwa Rating anime berkisar di angka 6-7

### Rating
"""

rating.info()

rating.describe()

rating

# cek duplikat
rating.duplicated().sum()

"""Data rating memiliki 1 duplikat"""

# cek missing value
rating.isnull().sum()

# Atur ukuran plot
plt.figure(figsize=(12, 6))

# Buat heatmap untuk menunjukkan nilai null
sns.heatmap(rating.isnull(),
            cmap='viridis',
            cbar=False,
            yticklabels=False
           )

plt.title("Heatmap Nilai Null dalam DataFrame Rating")
plt.show()

rating.hist(bins=50, figsize=(20,15))
plt.show()

"""Rating yang paling banyak diberikan user adalah 8

## Data Preprocessing

Sebelum dilakukan pengolahan data lebih jauh, data Anime digabung dengan data Rating berdasarkan fitur 'anime_id' untuk memudahkan pemrosesan karena menjadikannya satu dataframe.

### Menggabungkan Data dengan Fitur animeID
"""

data_merged = pd.merge(rating, anime, on='anime_id')

data_merged

data_merged.rename(columns={'rating_x': 'user_rating', 'rating_y': 'anime_rating'}, inplace=True)

data_merged

"""## Data Preparation

Tahap data preparation dilakukan untuk memastikan data siap digunakan dalam pelatihan model machine learning.

### Mengatasi Missing Value
"""

# Mengecek missing value pada dataframe data_merged
data_merged.isnull().sum()

"""Terdapat beberapa missing value"""

# Membersihkan missing value dengan fungsi dropna()
data_merged = data_merged.dropna()
data_merged

# Mengecek kembali missing value pada variabel data_merged
data_merged.isnull().sum()

"""### Mengurutkan berdasarkan anime_id

Setelah data bersih dari missing value, langkah selanjutnya adalah mengurutkan data berdasarkan kolom anime_id.
"""

data_merged = data_merged.sort_values('anime_id')
data_merged

"""### Drop Duplikat

Proses ini dilakukan untuk memastikan bahwa setiap anime hanya memiliki satu entri unik dalam data yang digunakan untuk rekomendasi.
"""

# Membuang data duplikat pada variabel preparation
preparation = data_merged.drop_duplicates('anime_id')
preparation

"""### Mengonversi data dalam bentuk list

Data kemudian diproses lebih lanjut ke dalam format list yang nantinya akan digunakan dalam proses pencocokan dan sistem rekomendasi berbasis konten (content-based filtering).
"""

# Mengonversi data series ‘anime_id’ menjadi dalam bentuk list
anime_id = preparation['anime_id'].tolist()

# Mengonversi data series ‘name’ menjadi dalam bentuk list
anime_name = preparation['name'].tolist()

# Mengonversi data series ‘genre’ menjadi dalam bentuk list
anime_genre = preparation['genre'].tolist()

print(len(anime_id))
print(len(anime_name))
print(len(anime_genre))

# Membuat dictionary untuk data ‘anime_id’, ‘anime_name’, dan 'anime_genre'
data = pd.DataFrame({
    'id': anime_id,
    'anime_name': anime_name,
    'genre': anime_genre
})
data

"""## Model Development dengan Content Based Filtering

Pendekatan pertama menggunakan teknik Content-Based Filtering dengan memanfaatkan informasi dari kolom genre setiap anime.
"""

data.sample(5)

"""### TF-IDF Vectorizer"""

# Buat TF-IDF matrix dari kolom genre
tfidf = TfidfVectorizer(token_pattern=r"[^,]+")  # anggap genre dipisah koma
tfidf.fit(data['genre'])
# Mapping array dari fitur index integer ke fitur nama
tfidf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf.fit_transform(data['genre'])
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan genre
# Baris diisi dengan nama anime

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=data.anime_name
).sample(22, axis=1).sample(10, axis=0)

"""### Cosine Similarity"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama anime
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['anime_name'], columns=data['anime_name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap anime
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### Mendapatkan Rekomendasi"""

def anime_recommendations(nama_anime, similarity_data=cosine_sim_df, items=data[['anime_name', 'genre']], k=10):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_anime].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_anime agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_anime, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

data[data.anime_name.eq('Fullmetal Alchemist: Brotherhood')]

# Mendapatkan rekomendasi anime yang mirip dengan Doubutsu Takarajima
anime_recommendations('Fullmetal Alchemist: Brotherhood')

"""Rekomendasi tersebut menghasilkan anime yang memiliki genre serupa dengan anime "Fullmetal Alchemist: Brotherhood"

## Model Development dengan Collaborative Filtering

Pendekatan kedua menggunakan Collaborative Filtering berbasis Neural Network, yaitu dengan membuat model deep learning kustom bernama RecommenderNet.

### Data Understanding
"""

# Membaca dataset
df = rating
df

"""### Data Preparation"""

# Hapus baris dengan rating -1
df = df[df['rating'] != -1]
df

# Mengubah user_id menjadi list tanpa nilai yang sama
user_ids = df['user_id'].unique().tolist()
print('list user_id: ', user_ids)

# Melakukan encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

# Mengubah anime_id menjadi list tanpa nilai yang sama
anime_ids = df['anime_id'].unique().tolist()

# Melakukan proses encoding anime_id
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}

# Melakukan proses encoding angka ke anime_id
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

# Mapping user_id ke dataframe user
df['user'] = df['user_id'].map(user_to_user_encoded)

# Mapping anime_id ke dataframe anime
df['anime'] = df['anime_id'].map(anime_to_anime_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah anime
num_anime = len(anime_encoded_to_anime)
print(num_anime)

# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])

# Nilai maksimal rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""### Membagi Data untuk Training dan Validasi"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan anime menjadi satu value
x = df[['user', 'anime']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""### Proses Training"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.anime_embedding = layers.Embedding( # layer embeddings anime
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1) # layer embedding anime bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    anime_vector = self.anime_embedding(inputs[:, 1]) # memanggil layer embedding 3
    anime_bias = self.anime_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

    x = dot_user_anime + user_bias + anime_bias

    return tf.nn.relu(x) # activation relu

model = RecommenderNet(num_users, num_anime, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.MeanSquaredError(),
    optimizer = keras.optimizers.Adam(learning_rate=1e-5),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    x_train, y_train,
    batch_size=1024,
    epochs=100,
    validation_data=(x_val, y_val),
    callbacks=[early_stop]
)

"""### Visualisasi Metrik"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Dari grafik hasil training model, terlihat bahwa nilai RMSE pada data training dan validation terus menurun dan stabil, menunjukkan bahwa model berhasil mempelajari representasi preferensi pengguna dengan cukup baik. Berikut adalah visualisasi metriknya:

### Mendapatkan Rekomendasi Anime
"""

anime_df = data
df = pd.read_csv('/kaggle/input/anime-recommendations-database/rating.csv')

# Mengambil sample user
user_id = df.user_id.sample(1).iloc[0]
anime_watched_by_user = df[df.user_id == user_id]

# Operator bitwise (~)
anime_not_watched = anime_df[~anime_df['id'].isin(anime_watched_by_user.anime_id.values)]['id']
anime_not_watched = list(
    set(anime_not_watched)
    .intersection(set(anime_to_anime_encoded.keys()))
)

anime_not_watched = [[anime_to_anime_encoded.get(x)] for x in anime_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)
)

ratings = model.predict(user_anime_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_watched[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('anime with high ratings from user')
print('----' * 8)

top_anime_user = (
    anime_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

anime_df_rows = anime_df[anime_df['id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.anime_name, ':', row.genre)

print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)

recommended_anime = anime_df[anime_df['id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.anime_name, ':', row.genre)

"""Prediksi rating terhadap anime yang belum ditonton untuk user tertentu menunjukkan hasil yang masuk akal, dan 10 rekomendasi teratas mencerminkan relevansi terhadap anime yang sebelumnya disukai oleh pengguna tersebut."""

